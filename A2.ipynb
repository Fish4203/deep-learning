{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14e2f6a-7351-4e21-b04a-dd3b7603de23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1fb0538",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# clean for debug purpose only \n",
    "\n",
    "trainData = None\n",
    "valData = None\n",
    "testData = None\n",
    "model = None\n",
    "\n",
    "tf.keras.backend.clear_session(free_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f2a32e",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c15f0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19573\n",
      "9274\n",
      "39129\n",
      "Counter({'entailment': 19619, 'contradiction': 19510})\n"
     ]
    }
   ],
   "source": [
    "# load images and labels \n",
    "\n",
    "\n",
    "labelSet = Counter()\n",
    "dataDict = {}\n",
    "vocab = set()\n",
    "\n",
    "\n",
    "datasetLen = 0\n",
    "with open(\"./A2_train_v3.jsonl\", \"r\") as jsonFile:\n",
    "\tfor line in jsonFile:\n",
    "\t\tdatasetLen += 1\n",
    "\t\tloadedLine = json.loads(line)\n",
    "\t\tif loadedLine[\"Image_ID\"] not in dataDict:\n",
    "\t\t\tdataDict[loadedLine[\"Image_ID\"]] = []\n",
    "\n",
    "\t\tlabelSet[loadedLine[\"Label\"]] += 1\n",
    "\n",
    "\t\thypo = [''.join(char for char in word if char.isalnum()) for word in loadedLine[\"Hypothesis\"].lower().split()]\n",
    "\t\tvocab.update(hypo)\n",
    "\n",
    "\t\tdataDict[loadedLine[\"Image_ID\"]].append((hypo, loadedLine[\"Label\"]))\n",
    "\n",
    "labelTuple = tuple(labelSet.keys())\n",
    "vocab = list(vocab)\n",
    "\n",
    "vocabIndex = {vocab[i]: i for i in range(len(vocab))}\n",
    "\n",
    "print(len(dataDict.keys()))\n",
    "print(len(vocabIndex))\n",
    "print(datasetLen)\n",
    "print(labelSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604c7f2",
   "metadata": {},
   "source": [
    "## Creating the tensorflow dataset\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d46b27a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded matrix\n"
     ]
    }
   ],
   "source": [
    "# load the glove embeddings \n",
    "\n",
    "def getGlove():\n",
    "  print('Downloading glove')\n",
    "  !wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "  !unzip -q glove.6B.zip\n",
    "\n",
    "def generateMatrix(dim):\n",
    "    print('parsing glove data')\n",
    "    embeddingMatrix = np.zeros((len(vocab), dim))\n",
    "    embeddedVocab = []\n",
    "    \n",
    "    with open(f'glove.6B.{dim}d.txt', encoding=\"utf-8\") as gloveFile:\n",
    "      for line in gloveFile:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "    \n",
    "        if word in vocab:\n",
    "          embeddingMatrix[vocabIndex[word]] = np.asarray(values[1:], dtype='float32')\n",
    "          embeddedVocab.append(word)\n",
    "\n",
    "    print(f'embedded {len(embeddedVocab)} out of {len(vocab)}')\n",
    "    return embeddingMatrix\n",
    "\n",
    "\n",
    "embeddingDim = 100\n",
    "embeddingMatrix = None\n",
    "\n",
    "if not os.path.isfile(f'glove.6B.{embeddingDim}d.txt'):\n",
    "  getGlove()\n",
    "\n",
    "if os.path.isfile(f'embeddingMatrix.{embeddingDim}d.pkl'):\n",
    "  with open(f'embeddingMatrix.{embeddingDim}d.pkl', 'rb') as f:\n",
    "    embeddingMatrix = pickle.load(f)\n",
    "\n",
    "  print('loaded matrix')\n",
    "else:\n",
    "  embeddingMatrix = generateMatrix(embeddingDim)\n",
    "\n",
    "  with open(f'embeddingMatrix.{embeddingDim}d.pkl', 'ab') as f:\n",
    "    pickle.dump(embeddingMatrix, f)\n",
    "\n",
    "  print('saved matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90abb6e6-b570-4164-ad36-1d5377b99bdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxLen = 128\n",
    "\n",
    "X1array = []\n",
    "X2array = []\n",
    "YArray = []\n",
    "\n",
    "for key, hypoAndLabels in dataDict.items():\n",
    "\timg = f'./A2_Images/{key}.jpg'\n",
    "\n",
    "\tfor hypo, label in hypoAndLabels:\n",
    "\t\tlabel = tf.convert_to_tensor([labelTuple.index(label)])\n",
    "\t\tlabel.set_shape([1])\n",
    "\n",
    "\t\thypo = [vocabIndex[word] for word in hypo]\n",
    "\t\thypo = tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences([hypo], maxlen=maxLen)[0])\n",
    "\t\thypo.set_shape([maxLen])\n",
    "\n",
    "\t\tX1array.append(img)\n",
    "\t\tX2array.append(hypo)\n",
    "\t\tYArray.append(label)\n",
    "\n",
    "X1Numpy = np.array(X1array)\n",
    "X2Numpy = np.array(X2array)\n",
    "YNumpy = np.array(YArray, dtype='uint8')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({'image': X1Numpy, 'text': X2Numpy}, YNumpy))\n",
    "\n",
    "def getImage(path):\n",
    "\timg = tf.io.read_file(path)\n",
    "\timg = tf.io.decode_image(img, channels=3)\n",
    "\timg = tf.image.resize(img, (224, 224), method='nearest')\n",
    "\treturn img\n",
    "\n",
    "def getImageWrapper(x, y):\n",
    "\timg = tf.py_function(func=getImage, inp=[x['image']], Tout=tf.uint8)\n",
    "\timg.set_shape([224, 224, 3])\n",
    "\n",
    "\tx['image'] = img \n",
    "\treturn x, y\n",
    "\n",
    "dataset = dataset.map(getImageWrapper, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc476692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'image': <tf.Tensor: shape=(224, 224, 3), dtype=uint8, numpy=\n",
      "array([[[ 55, 118, 162],\n",
      "        [ 53, 117, 165],\n",
      "        [ 45, 113, 162],\n",
      "        ...,\n",
      "        [179, 177, 164],\n",
      "        [175, 173, 161],\n",
      "        [175, 173, 160]],\n",
      "\n",
      "       [[ 61, 122, 169],\n",
      "        [ 54, 118, 166],\n",
      "        [ 53, 119, 169],\n",
      "        ...,\n",
      "        [180, 178, 165],\n",
      "        [174, 173, 155],\n",
      "        [176, 174, 159]],\n",
      "\n",
      "       [[ 58, 122, 168],\n",
      "        [ 57, 121, 169],\n",
      "        [ 54, 120, 170],\n",
      "        ...,\n",
      "        [177, 175, 162],\n",
      "        [179, 178, 158],\n",
      "        [176, 174, 159]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[129, 132, 141],\n",
      "        [149, 149, 157],\n",
      "        [168, 168, 170],\n",
      "        ...,\n",
      "        [157, 130, 123],\n",
      "        [158, 140, 130],\n",
      "        [155, 140, 133]],\n",
      "\n",
      "       [[  4,   5,  26],\n",
      "        [ 15,  12,  31],\n",
      "        [ 28,  24,  38],\n",
      "        ...,\n",
      "        [155, 128, 121],\n",
      "        [158, 139, 132],\n",
      "        [155, 140, 133]],\n",
      "\n",
      "       [[  8,   7,  12],\n",
      "        [ 12,   8,   7],\n",
      "        [ 17,  12,   8],\n",
      "        ...,\n",
      "        [150, 125, 118],\n",
      "        [156, 137, 130],\n",
      "        [150, 136, 127]]], dtype=uint8)>, 'text': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
      "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 7900,\n",
      "       5984, 3290, 5484, 6414, 4455, 7042, 6739], dtype=int32)>}, <tf.Tensor: shape=(1,), dtype=uint8, numpy=array([0], dtype=uint8)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 02:32:50.860475: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.take(1):\n",
    "\tprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "692ba090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data batches 20\n",
      "val data batches 20\n",
      "train data batches 353\n",
      "ratios test:1956 val:1956 train:35217\n"
     ]
    }
   ],
   "source": [
    "testSize = int(datasetLen * 0.05)\n",
    "valSize = int(datasetLen * 0.05)\n",
    "trainSize = int(datasetLen - testSize - valSize)\n",
    "batchSize = 100\n",
    "\n",
    "def optimize(ds):\n",
    "\tds = ds.batch(batchSize) \n",
    "\tds = ds.cache() \n",
    "\tds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\t\n",
    "\treturn ds\n",
    "\n",
    "def getTest(ds):\n",
    "\tds = ds.take(testSize) \n",
    "\tds = optimize(ds)\n",
    "\t\n",
    "\treturn ds\n",
    "\n",
    "def getVal(ds):\n",
    "\tds = ds.skip(testSize)\n",
    "\tds = ds.take(valSize) \n",
    "\tds = optimize(ds)\n",
    "\n",
    "\treturn ds\n",
    "\n",
    "def getTrain(ds):\n",
    "\tds = ds.skip(valSize + testSize)\n",
    "\tds = ds.take(trainSize)\n",
    "\tds = optimize(ds)\n",
    "\n",
    "\treturn ds\n",
    "\n",
    "testDS = getTest(dataset)\n",
    "valDS = getVal(dataset)\n",
    "trainDS = getTrain(dataset)\n",
    "\n",
    "\n",
    "print(f\"test data batches {tf.data.experimental.cardinality(testDS).numpy()}\")\n",
    "print(f\"val data batches {tf.data.experimental.cardinality(valDS).numpy()}\")\n",
    "print(f\"train data batches {tf.data.experimental.cardinality(trainDS).numpy()}\")\n",
    "print(f'ratios test:{testSize} val:{valSize} train:{trainSize}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a6a8b",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "ToDo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37e03aab-fb34-41c0-8006-0ee2da2d3297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n",
      "12683000/12683000 [==============================] - 2s 0us/step\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " text (InputLayer)           [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 128, 100)             927400    ['text[0][0]']                \n",
      "                                                                                                  \n",
      " denseText (TimeDistributed  (None, 128, 128)             12928     ['embedding[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " image (InputLayer)          [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " attentionText (MultiHeadAt  (None, 128, 128)             527488    ['denseText[0][0]',           \n",
      " tention)                                                            'denseText[0][0]',           \n",
      "                                                                     'denseText[0][0]']           \n",
      "                                                                                                  \n",
      " MobilenetV3large (Function  (None, 960)                  2996352   ['image[0][0]']               \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " denseText2 (Dense)          (None, 128, 256)             33024     ['attentionText[0][0]']       \n",
      "                                                                                                  \n",
      " denseImage (Dense)          (None, 256)                  246016    ['MobilenetV3large[0][0]']    \n",
      "                                                                                                  \n",
      " combined (Dot)              (None, 128)                  0         ['denseText2[0][0]',          \n",
      "                                                                     'denseImage[0][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 256)                  33024     ['combined[0][0]']            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 256)                  65792     ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " output (Dense)              (None, 1)                    257       ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4842281 (18.47 MB)\n",
      "Trainable params: 918529 (3.50 MB)\n",
      "Non-trainable params: 3923752 (14.97 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compiling\n",
    "inputImageLayer = tf.keras.layers.Input(shape=(224, 224, 3), name='image')\n",
    "\n",
    "# imagenet pre made model\n",
    "baseModel = tf.keras.applications.MobileNetV3Large(\n",
    "  weights='imagenet',\n",
    "  include_top=False,\n",
    "  pooling='max'\n",
    ")\n",
    "\n",
    "baseModel.trainable = False\n",
    "convoLayers = baseModel(inputImageLayer, training=False)\n",
    "denseImage = tf.keras.layers.Dense(256, activation='relu', name='denseImage')(convoLayers)\n",
    "\n",
    "inputTextLayer = tf.keras.layers.Input(shape=(maxLen,), name='text')\n",
    "embeddingText = tf.keras.layers.Embedding(len(vocab), embeddingDim, mask_zero=True, weights=[embeddingMatrix], trainable=False, name='embedding')(inputTextLayer)\n",
    "denseText = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(128, activation='relu'), name='denseText')(embeddingText)\n",
    "attentionText = tf.keras.layers.MultiHeadAttention(8, maxLen, name='attentionText')(denseText, denseText, denseText)\n",
    "denseText2 = tf.keras.layers.Dense(256, activation='relu', name='denseText2')(attentionText)\n",
    "\n",
    "combinedLayer = tf.keras.layers.Dot((2, 1), normalize=True, name='combined')([denseText2, denseImage])\n",
    "dense1 = tf.keras.layers.Dense(256, activation='relu')(combinedLayer)\n",
    "dense2 = tf.keras.layers.Dense(256, activation='relu')(dense1)\n",
    "output = tf.keras.layers.Dense(1, name='output', activation='sigmoid')(dense2)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputImageLayer, inputTextLayer], outputs=output)\n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "739d7062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 02:34:53.068683: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759804504.728324   13755 service.cc:145] XLA service 0x7f9b0dbba5d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1759804504.728357   13755 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-10-07 02:35:05.021926: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1759804505.419347   13755 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 170s 336ms/step - loss: 0.6207 - accuracy: 0.6296 - val_loss: 0.5578 - val_accuracy: 0.7188\n",
      "Epoch 2/8\n",
      "353/353 [==============================] - 44s 124ms/step - loss: 0.5208 - accuracy: 0.7332 - val_loss: 0.5241 - val_accuracy: 0.7265\n",
      "Epoch 3/8\n",
      "353/353 [==============================] - 44s 124ms/step - loss: 0.4960 - accuracy: 0.7519 - val_loss: 0.5151 - val_accuracy: 0.7306\n",
      "Epoch 4/8\n",
      "353/353 [==============================] - 44s 124ms/step - loss: 0.4796 - accuracy: 0.7621 - val_loss: 0.5112 - val_accuracy: 0.7393\n",
      "Epoch 5/8\n",
      "353/353 [==============================] - 44s 124ms/step - loss: 0.4636 - accuracy: 0.7724 - val_loss: 0.5054 - val_accuracy: 0.7403\n",
      "Epoch 6/8\n",
      "353/353 [==============================] - 44s 124ms/step - loss: 0.4450 - accuracy: 0.7832 - val_loss: 0.5012 - val_accuracy: 0.7413\n",
      "Epoch 7/8\n",
      "353/353 [==============================] - 44s 124ms/step - loss: 0.4284 - accuracy: 0.7930 - val_loss: 0.5007 - val_accuracy: 0.7444\n",
      "Epoch 8/8\n",
      "353/353 [==============================] - 44s 124ms/step - loss: 0.4102 - accuracy: 0.8023 - val_loss: 0.5191 - val_accuracy: 0.7382\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "history = model.fit(\n",
    "  trainDS,\n",
    "\tvalidation_data=valDS,\n",
    "  epochs=8,\n",
    "  batch_size=batchSize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "993c7e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./final.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
