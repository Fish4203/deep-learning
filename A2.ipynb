{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14e2f6a-7351-4e21-b04a-dd3b7603de23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 21:35:54.251462: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 21:35:56.469777: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.064968: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.065003: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.065987: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.066027: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.066046: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.066093: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.066113: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.066134: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.066147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23418 MB memory:  -> device: 0, name: AMD Radeon RX 7900 XTX, pci bus id: 0000:03:00.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1fb0538",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# clean for debug purpose only \n",
    "\n",
    "trainData = None\n",
    "valData = None\n",
    "testData = None\n",
    "model = None\n",
    "\n",
    "tf.keras.backend.clear_session(free_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f2a32e",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15f0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19573\n",
      "9274\n",
      "39129\n",
      "Counter({'entailment': 19619, 'contradiction': 19510})\n"
     ]
    }
   ],
   "source": [
    "# load images and labels \n",
    "\n",
    "\n",
    "labelSet = Counter()\n",
    "dataDict = {}\n",
    "vocab = set()\n",
    "\n",
    "\n",
    "datasetLen = 0\n",
    "with open(\"./A2_train_v3.jsonl\", \"r\") as jsonFile:\n",
    "\tfor line in jsonFile:\n",
    "\t\tdatasetLen += 1\n",
    "\t\tloadedLine = json.loads(line)\n",
    "\t\tif loadedLine[\"Image_ID\"] not in dataDict:\n",
    "\t\t\tdataDict[loadedLine[\"Image_ID\"]] = []\n",
    "\n",
    "\t\tlabelSet[loadedLine[\"Label\"]] += 1\n",
    "\n",
    "\t\thypo = [''.join(char for char in word if char.isalnum()) for word in loadedLine[\"Hypothesis\"].lower().split()]\n",
    "\t\tvocab.update(hypo)\n",
    "\n",
    "\t\tdataDict[loadedLine[\"Image_ID\"]].append((hypo, loadedLine[\"Label\"]))\n",
    "\n",
    "labelTuple = tuple(labelSet.keys())\n",
    "vocab = list(vocab)\n",
    "\n",
    "vocabIndex = {vocab[i]: i for i in range(len(vocab))}\n",
    "\n",
    "print(len(dataDict.keys()))\n",
    "print(len(vocabIndex))\n",
    "print(datasetLen)\n",
    "print(labelSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604c7f2",
   "metadata": {},
   "source": [
    "## Creating the tensorflow dataset\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d46b27a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded matrix\n"
     ]
    }
   ],
   "source": [
    "# load the glove embeddings \n",
    "\n",
    "def getGlove():\n",
    "  print('Downloading glove')\n",
    "  subprocess.run(['wget', 'https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip'])\n",
    "  subprocess.run(['unzip', '-q glove.6B.zip'])\n",
    "\n",
    "def generateMatrix(dim):\n",
    "    print('parsing glove data')\n",
    "    embeddingMatrix = np.zeros((len(vocab), dim))\n",
    "    embeddedVocab = []\n",
    "    \n",
    "    with open(f'glove.6B.{dim}d.txt', encoding=\"utf-8\") as gloveFile:\n",
    "      for line in gloveFile:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "    \n",
    "        if word in vocab:\n",
    "          embeddingMatrix[vocabIndex[word]] = np.asarray(values[1:], dtype='float32')\n",
    "          embeddedVocab.append(word)\n",
    "\n",
    "    print(f'embedded {len(embeddedVocab)} out of {len(vocab)}')\n",
    "    return embeddingMatrix\n",
    "\n",
    "\n",
    "embeddingDim = 200\n",
    "embeddingMatrix = None\n",
    "\n",
    "if not os.path.isfile(f'glove.6B.{embeddingDim}d.txt'):\n",
    "  getGlove()\n",
    "\n",
    "if os.path.isfile(f'embeddingMatrix.{embeddingDim}d.pkl'):\n",
    "  with open(f'embeddingMatrix.{embeddingDim}d.pkl', 'rb') as f:\n",
    "    embeddingMatrix = pickle.load(f)\n",
    "\n",
    "  print('loaded matrix')\n",
    "else:\n",
    "  embeddingMatrix = generateMatrix(embeddingDim)\n",
    "\n",
    "  with open(f'embeddingMatrix.{embeddingDim}d.pkl', 'ab') as f:\n",
    "    pickle.dump(embeddingMatrix, f)\n",
    "\n",
    "  print('saved matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "90abb6e6-b570-4164-ad36-1d5377b99bdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxLen = 128\n",
    "\n",
    "X1array = []\n",
    "X2array = []\n",
    "YArray = []\n",
    "\n",
    "for key, hypoAndLabels in dataDict.items():\n",
    "\timg = f'./A2_Images/{key}.jpg'\n",
    "\n",
    "\tfor hypo, label in hypoAndLabels:\n",
    "\t\tlabel = tf.convert_to_tensor([labelTuple.index(label)])\n",
    "\t\tlabel.set_shape([1])\n",
    "\n",
    "\t\thypo = [vocabIndex[word] for word in hypo]\n",
    "\t\thypo = tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences([hypo], maxlen=maxLen)[0])\n",
    "\t\thypo.set_shape([maxLen])\n",
    "\n",
    "\t\tX1array.append(img)\n",
    "\t\tX2array.append(hypo)\n",
    "\t\tYArray.append(label)\n",
    "\n",
    "X1Numpy = np.array(X1array)\n",
    "X2Numpy = np.array(X2array, dtype='uint16')\n",
    "YNumpy = np.array(YArray, dtype='uint8')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({'image': X1Numpy, 'text': X2Numpy}, YNumpy))\n",
    "\n",
    "def getImage(path):\n",
    "\timg = tf.io.read_file(path)\n",
    "\timg = tf.io.decode_image(img, channels=3, dtype=tf.uint8)\n",
    "\timg = tf.image.resize(img, (512, 512))\n",
    "\timg = tf.image.convert_image_dtype(img, dtype=tf.bfloat16)\n",
    "\timg = tf.reshape(img, (1, 512, 512, 3))\n",
    "\timg = tf.image.extract_patches(\n",
    "\t\timages=img,\n",
    "\t\tsizes=[1, 16, 16, 1],\n",
    "\t\tstrides=[1, 16, 16, 1],\n",
    "\t\trates=[1, 1, 1, 1],\n",
    "\t\tpadding='VALID'\n",
    "\t)\n",
    "\timg = tf.reshape(img, (32,32,768))\n",
    "\n",
    "\treturn img\n",
    "\n",
    "def getImageWrapper(x, y):\n",
    "\timg = tf.py_function(func=getImage, inp=[x['image']], Tout=tf.bfloat16)\n",
    "\timg.set_shape([32,32,768])\n",
    "\n",
    "\tx['image'] = img \n",
    "\treturn x, y\n",
    "\n",
    "dataset = dataset.map(getImageWrapper, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc482eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'image': <tf.Tensor: shape=(32, 32, 768), dtype=bfloat16, numpy=\n",
      "array([[[55, 116, 161, ..., 50.25, 114, 165],\n",
      "        [41.25, 111, 162, ..., 216, 208, 187],\n",
      "        [214, 212, 190, ..., 206, 199, 180],\n",
      "        ...,\n",
      "        [157, 70, 13.625, ..., 175, 176, 162],\n",
      "        [173, 171, 158, ..., 180, 178, 165],\n",
      "        [179, 180, 166, ..., 168, 166, 153]],\n",
      "\n",
      "       [[52.25, 118.5, 162, ..., 53, 120, 163],\n",
      "        [48, 114, 162, ..., 210, 204, 181],\n",
      "        [216, 209, 184, ..., 203, 202, 182],\n",
      "        ...,\n",
      "        [164, 29, 1.25, ..., 186, 188, 167],\n",
      "        [175, 176, 162, ..., 184, 182, 169],\n",
      "        [183, 181, 168, ..., 170, 168, 155]],\n",
      "\n",
      "       [[41.25, 105, 153, ..., 118, 155, 178],\n",
      "        [58.5, 122, 166, ..., 151, 154, 145],\n",
      "        [215, 208, 184, ..., 191, 192, 171],\n",
      "        ...,\n",
      "        [190, 52.75, 5.28125, ..., 181, 179, 166],\n",
      "        [187, 188, 169, ..., 187, 185, 172],\n",
      "        [186, 184, 171, ..., 168, 166, 153]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[157, 157, 142, ..., 173, 172, 160],\n",
      "        [83.5, 80.5, 73.5, ..., 152, 156, 142],\n",
      "        [63.75, 63, 58, ..., 123, 141, 130],\n",
      "        ...,\n",
      "        [30, 31, 25.375, ..., 33, 34.5, 33.75],\n",
      "        [32.25, 36, 30.75, ..., 165, 104, 136],\n",
      "        [221, 49, 61, ..., 198, 194, 205]],\n",
      "\n",
      "       [[155, 155, 150, ..., 187, 185, 171],\n",
      "        [161, 159, 146, ..., 177, 175, 162],\n",
      "        [175, 173, 159, ..., 155, 155, 140],\n",
      "        ...,\n",
      "        [33.75, 33.5, 35.75, ..., 38.75, 36.75, 30.125],\n",
      "        [35.5, 38.75, 36.25, ..., 164, 126, 118],\n",
      "        [197, 95, 114, ..., 155, 126, 118]],\n",
      "\n",
      "       [[182, 180, 169, ..., 78.5, 38.5, 18.875],\n",
      "        [183, 181, 170, ..., 74.5, 42.5, 31.5],\n",
      "        [173, 171, 160, ..., 149, 149, 141],\n",
      "        ...,\n",
      "        [32.75, 33.5, 33.25, ..., 157, 52.75, 72.5],\n",
      "        [31.75, 30.625, 36.25, ..., 180, 52.5, 70],\n",
      "        [164, 129, 122.5, ..., 153, 139, 130]]], dtype=bfloat16)>, 'text': <tf.Tensor: shape=(128,), dtype=uint16, numpy=\n",
      "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 5401,\n",
      "        193, 1840, 3350, 3606, 2326, 4552, 7595], dtype=uint16)>}, <tf.Tensor: shape=(1,), dtype=uint8, numpy=array([0], dtype=uint8)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 22:12:02.576742: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.take(1):\n",
    "\tprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50e3da53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.8390292  0.7480758  0.90529305 ... 0.79180294 0.9660142  0.04925289]\n",
      "  [0.88415784 0.12577565 0.28305644 ... 0.908674   0.7082081  0.07784933]\n",
      "  [0.14504912 0.8904532  0.25477633 ... 0.5163142  0.61102545 0.63277656]\n",
      "  [0.4768772  0.7293987  0.89062566 ... 0.31080365 0.19454598 0.73740184]]\n",
      "\n",
      " [[0.04842119 0.78201604 0.5962284  ... 0.393729   0.64133334 0.30180177]\n",
      "  [0.74369717 0.80858225 0.8433     ... 0.6428573  0.7747854  0.8645563 ]\n",
      "  [0.35362944 0.45882687 0.21380596 ... 0.83703375 0.08469753 0.13021818]\n",
      "  [0.41641042 0.12799005 0.35094154 ... 0.36266577 0.86535305 0.18672238]]\n",
      "\n",
      " [[0.36918664 0.23469527 0.29032108 ... 0.21911213 0.06178004 0.77791804]\n",
      "  [0.87476987 0.68608606 0.0249742  ... 0.12947448 0.14006604 0.9795229 ]\n",
      "  [0.50127435 0.38305467 0.7084395  ... 0.1350239  0.37582183 0.73694515]\n",
      "  [0.36680695 0.8297529  0.6200732  ... 0.6234115  0.6546367  0.80751973]]\n",
      "\n",
      " [[0.46511182 0.80500436 0.09670209 ... 0.12187485 0.54915303 0.60303885]\n",
      "  [0.23125681 0.80721    0.9758009  ... 0.07497837 0.8803603  0.39834246]\n",
      "  [0.80006135 0.5635453  0.68618035 ... 0.5403606  0.87199473 0.09868617]\n",
      "  [0.67294705 0.2918821  0.18311931 ... 0.0856793  0.16922821 0.7560458 ]]], shape=(4, 4, 256), dtype=float32)\n",
      "[[[0.8390292  0.7480758  0.90529305 0.81581473]\n",
      "  [0.6664157  0.5996901  0.28758976 0.23698677]\n",
      "  [0.63961667 0.8446172  0.20324007 0.81819177]\n",
      "  [0.6921186  0.0909567  0.7073609  0.9390378 ]]\n",
      "\n",
      " [[0.68300444 0.35200605 0.76105046 0.6842206 ]\n",
      "  [0.85281485 0.71869344 0.61226016 0.68272936]\n",
      "  [0.8153009  0.93814754 0.38457376 0.8767886 ]\n",
      "  [0.02797676 0.774335   0.6558154  0.5010969 ]]\n",
      "\n",
      " [[0.1074701  0.72447854 0.93389004 0.91978794]\n",
      "  [0.2896515  0.7949469  0.7699212  0.5667323 ]\n",
      "  [0.1038155  0.2737905  0.75986433 0.5770567 ]\n",
      "  [0.96457785 0.19328293 0.91237116 0.4172025 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.06663018 0.38998613 0.6157484  0.34234303]\n",
      "  [0.9536172  0.9772312  0.32197973 0.08956985]\n",
      "  [0.2644883  0.9873735  0.64073193 0.6956252 ]\n",
      "  [0.27017614 0.4380175  0.585603   0.18204905]]\n",
      "\n",
      " [[0.30426267 0.46652982 0.6097755  0.5267822 ]\n",
      "  [0.88779426 0.133656   0.5541188  0.70677286]\n",
      "  [0.02300166 0.22708215 0.545009   0.28457087]\n",
      "  [0.34322405 0.3782985  0.5255363  0.6848094 ]]\n",
      "\n",
      " [[0.45095208 0.9824578  0.8320235  0.26135594]\n",
      "  [0.13061428 0.22702931 0.7562559  0.18821691]\n",
      "  [0.6508388  0.40847397 0.8942166  0.5381505 ]\n",
      "  [0.5585779  0.0856793  0.16922821 0.7560458 ]]]\n"
     ]
    }
   ],
   "source": [
    "image = np.random.random((64, 64, 1)).astype(\"float32\") # 1 RGB image\n",
    "patches = tf.keras.ops.image.extract_patches(image, (16, 16))\n",
    "\n",
    "print(patches)\n",
    "print(tf.reshape(patches, (256, 4, 4)).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "692ba090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data batches 20\n",
      "val data batches 20\n",
      "train data batches 353\n",
      "ratios test:1956 val:1956 train:35217\n"
     ]
    }
   ],
   "source": [
    "testSize = int(datasetLen * 0.05)\n",
    "valSize = int(datasetLen * 0.05)\n",
    "trainSize = int(datasetLen - testSize - valSize)\n",
    "batchSize = 100\n",
    "\n",
    "def optimize(ds):\n",
    "\tds = ds.batch(batchSize) \n",
    "\tds = ds.cache() \n",
    "\tds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\t\n",
    "\treturn ds\n",
    "\n",
    "def getTest(ds):\n",
    "\tds = ds.take(testSize) \n",
    "\tds = optimize(ds)\n",
    "\t\n",
    "\treturn ds\n",
    "\n",
    "def getVal(ds):\n",
    "\tds = ds.skip(testSize)\n",
    "\tds = ds.take(valSize) \n",
    "\tds = optimize(ds)\n",
    "\n",
    "\treturn ds\n",
    "\n",
    "def getTrain(ds):\n",
    "\tds = ds.skip(valSize + testSize)\n",
    "\tds = ds.take(trainSize)\n",
    "\tds = optimize(ds)\n",
    "\n",
    "\treturn ds\n",
    "\n",
    "testDS = getTest(dataset)\n",
    "valDS = getVal(dataset)\n",
    "trainDS = getTrain(dataset)\n",
    "\n",
    "\n",
    "print(f\"test data batches {tf.data.experimental.cardinality(testDS).numpy()}\")\n",
    "print(f\"val data batches {tf.data.experimental.cardinality(valDS).numpy()}\")\n",
    "print(f\"train data batches {tf.data.experimental.cardinality(trainDS).numpy()}\")\n",
    "print(f'ratios test:{testSize} val:{valSize} train:{trainSize}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a6a8b",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "ToDo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e03aab-fb34-41c0-8006-0ee2da2d3297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/archfishgame/deep/deep-learning/.venv/lib/python3.10/site-packages/keras/src/applications/mobilenet_v3.py:517: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  return MobileNetV3(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'denseText' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m inputTextLayer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(maxLen,), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m embeddingText \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mEmbedding(\u001b[38;5;28mlen\u001b[39m(vocab), embeddingDim, mask_zero\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, weights\u001b[38;5;241m=\u001b[39m[embeddingMatrix], trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m)(inputTextLayer)\n\u001b[0;32m---> 22\u001b[0m attentionText \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mMultiHeadAttention(\u001b[38;5;241m8\u001b[39m, maxLen, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattentionText\u001b[39m\u001b[38;5;124m'\u001b[39m, output_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m))(\u001b[43mdenseText\u001b[49m, denseText, denseText)\n\u001b[1;32m     23\u001b[0m attentionText \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mMultiHeadAttention(\u001b[38;5;241m8\u001b[39m, maxLen, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattentionText\u001b[39m\u001b[38;5;124m'\u001b[39m, output_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m))(denseText, denseText, denseText)\n\u001b[1;32m     25\u001b[0m textOut \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLayerNormalization()(attentionText)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'denseText' is not defined"
     ]
    }
   ],
   "source": [
    "# Image Encoder \n",
    "class PatchEncoder(Layer):\n",
    "    def __init__(self, num_patches=196, projection_dim=768):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection_dim = projection_dim\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        class_token = w_init(shape=(1, projection_dim), dtype=\"float32\")\n",
    "        self.class_token = tf.Variable(initial_value=class_token, trainable=True)\n",
    "        self.projection = Dense(units=projection_dim)\n",
    "        self.position_embedding = Embedding(input_dim=num_patches+1, output_dim=projection_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        batch = tf.shape(patch)[0]\n",
    "        # reshape the class token embedins\n",
    "        class_token = tf.tile(self.class_token, multiples = [batch, 1])\n",
    "        class_token = tf.reshape(class_token, (batch, 1, self.projection_dim))\n",
    "        # calculate patches embeddings\n",
    "        patches_embed = self.projection(patch)\n",
    "        patches_embed = tf.concat([patches_embed, class_token], 1)\n",
    "        # calcualte positional embeddings\n",
    "        positions = tf.range(start=0, limit=self.num_patches+1, delta=1)\n",
    "        positions_embed = self.position_embedding(positions)\n",
    "        # add both embeddings\n",
    "        encoded = patches_embed + positions_embed\n",
    "        return encoded\n",
    "\n",
    "inputImageLayer = tf.keras.layers.Input(shape=(224, 224, 3), name='image')\n",
    "\n",
    "\n",
    "# imagenet pre made model\n",
    "baseModel = tf.keras.applications.MobileNetV3Large(\n",
    "  weights='imagenet',\n",
    "  include_top=False,\n",
    "  pooling='max'\n",
    ")\n",
    "\n",
    "baseModel.trainable = False\n",
    "convoLayers = baseModel(inputImageLayer, training=False)\n",
    "denseImage = tf.keras.layers.Dense(256, activation='relu', name='denseImage')(convoLayers)\n",
    "\n",
    "imageOut = tf.keras.layers.LayerNormalization()(denseImage)\n",
    "\n",
    "\n",
    "# Text Encoder \n",
    "\n",
    "inputTextLayer = tf.keras.layers.Input(shape=(maxLen,), name='text')\n",
    "embeddingText = tf.keras.layers.Embedding(len(vocab), embeddingDim, mask_zero=True, weights=[embeddingMatrix], trainable=False, name='embedding')(inputTextLayer)\n",
    "attentionText = tf.keras.layers.MultiHeadAttention(8, maxLen, name='attentionText', output_shape=(256))(denseText, denseText, denseText)\n",
    "attentionText = tf.keras.layers.MultiHeadAttention(8, maxLen, name='attentionText', output_shape=(256))(denseText, denseText, denseText)\n",
    "\n",
    "textOut = tf.keras.layers.LayerNormalization()(attentionText)\n",
    "\n",
    "# Entanglement decoder \n",
    "decoderInput = tf.keras.layers.Concatenate()([imageOut, textOut])\n",
    "\n",
    "dense1 = tf.keras.layers.Dense(256, activation='relu')(decoderInput)\n",
    "dense2 = tf.keras.layers.Dense(256, activation='relu')(dense1)\n",
    "\n",
    "decoderOutput = tf.keras.layers.Dense(1, name='output', activation='sigmoid')(dense2)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputImageLayer, inputTextLayer], outputs=decoderOutput)\n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d7062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "history = model.fit(\n",
    "  trainDS,\n",
    "\tvalidation_data=valDS,\n",
    "  epochs=8,\n",
    "  batch_size=batchSize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c7e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./final.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
