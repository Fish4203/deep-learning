{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e2f6a-7351-4e21-b04a-dd3b7603de23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 13:14:31.660609: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 13:14:34.557427: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 13:14:37.100758: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-10 13:14:37.100798: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-10 13:14:37.101569: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-10 13:14:37.101620: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-10 13:14:37.101641: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-10 13:14:37.101690: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-10 13:14:37.101711: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-10 13:14:37.101734: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-10 13:14:37.101748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23418 MB memory:  -> device: 0, name: AMD Radeon RX 7900 XTX, pci bus id: 0000:03:00.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import json\n",
    "import subprocess\n",
    "from vit_keras import vit \n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f2a32e",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15f0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19573\n",
      "9274\n",
      "39129\n",
      "Counter({'entailment': 19619, 'contradiction': 19510})\n"
     ]
    }
   ],
   "source": [
    "# load images and labels \n",
    "\n",
    "\n",
    "labelSet = Counter()\n",
    "dataDict = {}\n",
    "vocab = set()\n",
    "\n",
    "\n",
    "datasetLen = 0\n",
    "with open(\"./A2_train_v3.jsonl\", \"r\") as jsonFile:\n",
    "\tfor line in jsonFile:\n",
    "\t\tdatasetLen += 1\n",
    "\t\tloadedLine = json.loads(line)\n",
    "\t\tif loadedLine[\"Image_ID\"] not in dataDict:\n",
    "\t\t\tdataDict[loadedLine[\"Image_ID\"]] = []\n",
    "\n",
    "\t\tlabelSet[loadedLine[\"Label\"]] += 1\n",
    "\n",
    "\t\thypo = [''.join(char for char in word if char.isalnum()) for word in loadedLine[\"Hypothesis\"].lower().split()]\n",
    "\t\tvocab.update(hypo)\n",
    "\n",
    "\t\tdataDict[loadedLine[\"Image_ID\"]].append((hypo, loadedLine[\"Label\"]))\n",
    "\n",
    "labelTuple = tuple(labelSet.keys())\n",
    "vocab = list(vocab)\n",
    "\n",
    "vocabIndex = {vocab[i]: i for i in range(len(vocab))}\n",
    "\n",
    "print(len(dataDict.keys()))\n",
    "print(len(vocabIndex))\n",
    "print(datasetLen)\n",
    "print(labelSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604c7f2",
   "metadata": {},
   "source": [
    "## Creating the tensorflow dataset\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d46b27a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded matrix\n"
     ]
    }
   ],
   "source": [
    "# load the glove embeddings \n",
    "\n",
    "def getGlove():\n",
    "  print('Downloading glove')\n",
    "  subprocess.run(['wget', 'https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip'])\n",
    "  subprocess.run(['unzip', '-q glove.6B.zip'])\n",
    "\n",
    "def generateMatrix(dim):\n",
    "    print('parsing glove data')\n",
    "    embeddingMatrix = np.zeros((len(vocab), dim))\n",
    "    embeddedVocab = []\n",
    "    \n",
    "    with open(f'glove.6B.{dim}d.txt', encoding=\"utf-8\") as gloveFile:\n",
    "      for line in gloveFile:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "    \n",
    "        if word in vocab:\n",
    "          embeddingMatrix[vocabIndex[word]] = np.asarray(values[1:], dtype='float32')\n",
    "          embeddedVocab.append(word)\n",
    "\n",
    "    print(f'embedded {len(embeddedVocab)} out of {len(vocab)}')\n",
    "    return embeddingMatrix\n",
    "\n",
    "\n",
    "embeddingDim = 200\n",
    "embeddingMatrix = None\n",
    "\n",
    "if not os.path.isfile(f'glove.6B.{embeddingDim}d.txt'):\n",
    "  getGlove()\n",
    "\n",
    "if os.path.isfile(f'embeddingMatrix.{embeddingDim}d.pkl'):\n",
    "  with open(f'embeddingMatrix.{embeddingDim}d.pkl', 'rb') as f:\n",
    "    embeddingMatrix = pickle.load(f)\n",
    "\n",
    "  print('loaded matrix')\n",
    "else:\n",
    "  embeddingMatrix = generateMatrix(embeddingDim)\n",
    "\n",
    "  with open(f'embeddingMatrix.{embeddingDim}d.pkl', 'ab') as f:\n",
    "    pickle.dump(embeddingMatrix, f)\n",
    "\n",
    "  print('saved matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90abb6e6-b570-4164-ad36-1d5377b99bdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxLen = 128\n",
    "\n",
    "X1array = []\n",
    "X2array = []\n",
    "YArray = []\n",
    "\n",
    "for key, hypoAndLabels in dataDict.items():\n",
    "\timg = f'./A2_Images/{key}.jpg'\n",
    "\n",
    "\tfor hypo, label in hypoAndLabels:\n",
    "\t\tlabel = tf.convert_to_tensor([labelTuple.index(label)])\n",
    "\t\tlabel.set_shape([1])\n",
    "\n",
    "\t\thypo = [vocabIndex[word] for word in hypo]\n",
    "\t\thypo = tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences([hypo], maxlen=maxLen)[0])\n",
    "\t\thypo.set_shape([maxLen])\n",
    "\n",
    "\t\tX1array.append(img)\n",
    "\t\tX2array.append(hypo)\n",
    "\t\tYArray.append(label)\n",
    "\n",
    "X1Numpy = np.array(X1array)\n",
    "X2Numpy = np.array(X2array, dtype='uint16')\n",
    "YNumpy = np.array(YArray, dtype='uint8')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({'image': X1Numpy, 'text': X2Numpy}, YNumpy))\n",
    "\n",
    "imageChannels = 3\n",
    "imageRes = 256\n",
    "patchesPerImage = 8\n",
    "\n",
    "patchRes = int(imageRes / patchesPerImage) \n",
    "patchNum = patchesPerImage ** 2 * imageChannels \n",
    "\n",
    "def getImage(path):\n",
    "\timg = tf.io.read_file(path)\n",
    "\timg = tf.io.decode_image(img, channels=imageChannels, dtype=tf.float32)\n",
    "\timg = tf.image.resize(img, (imageRes, imageRes))\n",
    "\n",
    "\treturn img\n",
    "\n",
    "def getExtractPatches(img):\n",
    "\timg = tf.expand_dims(img, axis=0)\n",
    "\timg = tf.image.extract_patches(\n",
    "\t\timages=img,\n",
    "\t\tsizes=[1, patchesPerImage, patchesPerImage, 1],\n",
    "\t\tstrides=[1, patchesPerImage, patchesPerImage, 1],\n",
    "\t\trates=[1, 1, 1, 1],\n",
    "\t\tpadding='VALID'\n",
    "\t)\n",
    "\timg = tf.reshape(img, (patchRes ** 2, patchNum))\n",
    "\n",
    "\treturn img\n",
    "\n",
    "def getImageWrapper(x, y):\n",
    "\timg = tf.py_function(func=getImage, inp=[x['image']], Tout=tf.float32)\n",
    "\timg.set_shape([imageRes, imageRes, imageChannels])\n",
    "\t# img = tf.py_function(func=getExtractPatches, inp=[img], Tout=tf.float32)\n",
    "\t# img.set_shape([patchRes ** 2, patchNum])\n",
    "\n",
    "\tx['image'] = img \n",
    "\treturn x, y\n",
    "\n",
    "dataset = dataset.map(getImageWrapper, num_parallel_calls=8)\n",
    "# dataset.save('cachedDataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc482eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.load('cachedDataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "692ba090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data batches 20\n",
      "val data batches 20\n",
      "train data batches 353\n",
      "ratios test:1956 val:1956 train:35217\n"
     ]
    }
   ],
   "source": [
    "testSize = int(datasetLen * 0.05)\n",
    "valSize = int(datasetLen * 0.05)\n",
    "trainSize = int(datasetLen - testSize - valSize)\n",
    "batchSize = 100\n",
    "\n",
    "def optimize(ds):\n",
    "\tds = ds.batch(batchSize) \n",
    "\tds = ds.cache() \n",
    "\tds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\t\n",
    "\treturn ds\n",
    "\n",
    "def getTest(ds):\n",
    "\tds = ds.take(testSize) \n",
    "\tds = optimize(ds)\n",
    "\t\n",
    "\treturn ds\n",
    "\n",
    "def getVal(ds):\n",
    "\tds = ds.skip(testSize)\n",
    "\tds = ds.take(valSize) \n",
    "\tds = optimize(ds)\n",
    "\n",
    "\treturn ds\n",
    "\n",
    "def getTrain(ds):\n",
    "\tds = ds.skip(valSize + testSize)\n",
    "\tds = ds.take(trainSize)\n",
    "\tds = optimize(ds)\n",
    "\n",
    "\treturn ds\n",
    "\n",
    "testDS = getTest(dataset)\n",
    "valDS = getVal(dataset)\n",
    "trainDS = getTrain(dataset)\n",
    "\n",
    "\n",
    "print(f\"test data batches {tf.data.experimental.cardinality(testDS).numpy()}\")\n",
    "print(f\"val data batches {tf.data.experimental.cardinality(valDS).numpy()}\")\n",
    "print(f\"train data batches {tf.data.experimental.cardinality(trainDS).numpy()}\")\n",
    "print(f'ratios test:{testSize} val:{valSize} train:{trainSize}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a6a8b",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "ToDo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37e03aab-fb34-41c0-8006-0ee2da2d3297",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'vit_keras' has no attribute 'vit_b16'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m inputImageLayer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(imageRes, imageRes, imageChannels), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# imagenet pre made model\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m baseModel \u001b[38;5;241m=\u001b[39m \u001b[43mvit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit_b16\u001b[49m(\n\u001b[1;32m     54\u001b[0m     image_size\u001b[38;5;241m=\u001b[39mimage_size,\n\u001b[1;32m     55\u001b[0m     pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m     include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     59\u001b[0m baseModel\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     60\u001b[0m pretrainedLayers \u001b[38;5;241m=\u001b[39m baseModel(inputLayer, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'vit_keras' has no attribute 'vit_b16'"
     ]
    }
   ],
   "source": [
    "denseStrides = 64\n",
    "\n",
    "class PositionEncoder(tf.keras.Layer):\n",
    "\tdef __init__(self, patchRes, patchNum, name=None):\n",
    "\t\tsuper(PositionEncoder, self).__init__(name=name)\n",
    "\t\tself.patchRes = patchRes\n",
    "\t\tself.patchNum = patchNum\n",
    "\t\tw_init = tf.random_normal_initializer()\n",
    "\t\tclassToken = w_init(shape=(1, patchNum), dtype=tf.float32)\n",
    "\t\tself.classToken = tf.Variable(initial_value=classToken, trainable=True)\n",
    "\t\tself.projection =tf.keras.layers.Dense(units=patchNum)\n",
    "\t\tself.positionEmbedding = tf.keras.layers.Embedding(input_dim=patchRes+1, output_dim=patchNum, mask_zero=True)\n",
    "\n",
    "\tdef call(self, patch):\n",
    "\t\tbatch = tf.shape(patch)[0]\n",
    "\t\tclassToken = tf.tile(self.classToken, multiples = [batch, 1])\n",
    "\t\tclassToken = tf.reshape(classToken, (batch, 1, self.patchNum))\n",
    "\t\tpatchesEmbed = self.projection(patch)\n",
    "\t\tpatchesEmbed = tf.concat([patchesEmbed, classToken], 1)\n",
    "\t\tpositions = tf.range(start=0, limit=self.patchRes+1, delta=1)\n",
    "\t\tpositionsEmbed = self.positionEmbedding(positions)\n",
    "\t\tencoded = patchesEmbed + positionsEmbed\n",
    "\t\treturn encoded\n",
    "\n",
    "class Transformer(tf.keras.Layer):\n",
    "\tdef __init__(self, dim, denseSize, heads=4, dropout=0.1, name=None):\n",
    "\t\tsuper(Transformer, self).__init__(name=name)\n",
    "\t\tself.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\t\tself.attn = tf.keras.layers.MultiHeadAttention(num_heads=heads, key_dim=dim, dropout=dropout)\n",
    "\t\tself.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\t\tself.dense1 = tf.keras.layers.Dense(denseSize, activation='relu')\n",
    "\t\tself.dense2 = tf.keras.layers.Dense(dim, activation='relu')\n",
    "\t\tself.drop = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "\tdef call(self, inputLayers):\n",
    "\t\tattention = self.attn(inputLayers, inputLayers)\n",
    "\t\tattention = tf.keras.layers.Add()([attention, inputLayers])\n",
    "\t\tattention = self.norm1(attention)\n",
    "\n",
    "\t\tmlp = self.drop(attention)\n",
    "\t\tmlp = self.dense1(mlp)\n",
    "\t\tmlp = self.drop(mlp)\n",
    "\t\tmlp = self.dense2(mlp)\n",
    "\t\tmlp = tf.keras.layers.Add()([mlp, attention])\n",
    "\t\tmlp = self.norm2(mlp)\n",
    "\n",
    "\t\treturn mlp\n",
    "\n",
    "# Image Encoder \n",
    "inputImageLayer = tf.keras.layers.Input(shape=(imageRes, imageRes, imageChannels), name='image')\n",
    "\n",
    "# imagenet pre made model\n",
    "baseModel = vit.vit_b16(\n",
    "    image_size=image_size,\n",
    "    pretrained=True,\n",
    "    include_top=False\n",
    ")\n",
    "\n",
    "baseModel.trainable = False\n",
    "pretrainedLayers = baseModel(inputLayer, training=False)\n",
    "\n",
    "imageOut = tf.keras.layers.Dense(denseStrides, activation='relu', name='imageOut')(pretrainedLayers)\n",
    "\n",
    "\n",
    "# Text Encoder \n",
    "inputTextLayer = tf.keras.layers.Input(shape=(maxLen,), name='text')\n",
    "\n",
    "embeddingText = tf.keras.layers.Embedding(len(vocab), embeddingDim, weights=[embeddingMatrix], trainable=False)(inputTextLayer)\n",
    "reshapeText = tf.keras.layers.Reshape((embeddingDim, maxLen))(embeddingText)\n",
    "embeddingPositionText = PositionEncoder(embeddingDim, maxLen, name='embeddingText')(reshapeText)\n",
    "\n",
    "transformerText = Transformer(maxLen, 128)(embeddingPositionText)\n",
    "\n",
    "textOut = tf.keras.layers.Dense(denseStrides, activation='relu', name='textOut')(transformerText)\n",
    "\n",
    "# Entanglement decoder \n",
    "decoderInput = tf.keras.layers.Concatenate(-2, name='decoderInput')([imageOut, textOut])\n",
    "\n",
    "transformerDecode = Transformer(denseStrides, 128)(decoderInput)\n",
    "\n",
    "flat = tf.keras.layers.GlobalAveragePooling1D()(transformerDecode)\n",
    "dense = tf.keras.layers.Dense(256, activation='relu')(flat)\n",
    "decoderOutput = tf.keras.layers.Dense(1, name='output', activation='sigmoid')(dense)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputImageLayer, inputTextLayer], outputs=decoderOutput)\n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d7062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760061380.561783  469457 service.cc:145] XLA service 0x73410480c1f0 initialized for platform ROCM (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1760061380.561806  469457 service.cc:153]   StreamExecutor device (0): AMD Radeon RX 7900 XTX, AMDGPU ISA version: gfx1100\n",
      "2025-10-10 12:56:20.658401: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1760061473.333598  469457 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-10-10 12:57:53.418835: W external/local_tsl/tsl/framework/bfc_allocator.cc:368] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m352/353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.5351 - loss: 0.6913"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "history = model.fit(\n",
    "  trainDS,\n",
    "\tvalidation_data=valDS,\n",
    "  epochs=16,\n",
    "  batch_size=batchSize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "993c7e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('a2-models/firstGood.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb15df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
