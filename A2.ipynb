{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14e2f6a-7351-4e21-b04a-dd3b7603de23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 21:35:54.251462: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 21:35:56.469777: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.064968: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.065003: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.065987: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.066027: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.066046: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.066093: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.066113: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.066134: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:926] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-10-08 21:36:00.066147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23418 MB memory:  -> device: 0, name: AMD Radeon RX 7900 XTX, pci bus id: 0000:03:00.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1fb0538",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# clean for debug purpose only \n",
    "\n",
    "trainData = None\n",
    "valData = None\n",
    "testData = None\n",
    "model = None\n",
    "\n",
    "tf.keras.backend.clear_session(free_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f2a32e",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15f0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19573\n",
      "9274\n",
      "39129\n",
      "Counter({'entailment': 19619, 'contradiction': 19510})\n"
     ]
    }
   ],
   "source": [
    "# load images and labels \n",
    "\n",
    "\n",
    "labelSet = Counter()\n",
    "dataDict = {}\n",
    "vocab = set()\n",
    "\n",
    "\n",
    "datasetLen = 0\n",
    "with open(\"./A2_train_v3.jsonl\", \"r\") as jsonFile:\n",
    "\tfor line in jsonFile:\n",
    "\t\tdatasetLen += 1\n",
    "\t\tloadedLine = json.loads(line)\n",
    "\t\tif loadedLine[\"Image_ID\"] not in dataDict:\n",
    "\t\t\tdataDict[loadedLine[\"Image_ID\"]] = []\n",
    "\n",
    "\t\tlabelSet[loadedLine[\"Label\"]] += 1\n",
    "\n",
    "\t\thypo = [''.join(char for char in word if char.isalnum()) for word in loadedLine[\"Hypothesis\"].lower().split()]\n",
    "\t\tvocab.update(hypo)\n",
    "\n",
    "\t\tdataDict[loadedLine[\"Image_ID\"]].append((hypo, loadedLine[\"Label\"]))\n",
    "\n",
    "labelTuple = tuple(labelSet.keys())\n",
    "vocab = list(vocab)\n",
    "\n",
    "vocabIndex = {vocab[i]: i for i in range(len(vocab))}\n",
    "\n",
    "print(len(dataDict.keys()))\n",
    "print(len(vocabIndex))\n",
    "print(datasetLen)\n",
    "print(labelSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604c7f2",
   "metadata": {},
   "source": [
    "## Creating the tensorflow dataset\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d46b27a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded matrix\n"
     ]
    }
   ],
   "source": [
    "# load the glove embeddings \n",
    "\n",
    "def getGlove():\n",
    "  print('Downloading glove')\n",
    "  subprocess.run(['wget', 'https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip'])\n",
    "  subprocess.run(['unzip', '-q glove.6B.zip'])\n",
    "\n",
    "def generateMatrix(dim):\n",
    "    print('parsing glove data')\n",
    "    embeddingMatrix = np.zeros((len(vocab), dim))\n",
    "    embeddedVocab = []\n",
    "    \n",
    "    with open(f'glove.6B.{dim}d.txt', encoding=\"utf-8\") as gloveFile:\n",
    "      for line in gloveFile:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "    \n",
    "        if word in vocab:\n",
    "          embeddingMatrix[vocabIndex[word]] = np.asarray(values[1:], dtype='float32')\n",
    "          embeddedVocab.append(word)\n",
    "\n",
    "    print(f'embedded {len(embeddedVocab)} out of {len(vocab)}')\n",
    "    return embeddingMatrix\n",
    "\n",
    "\n",
    "embeddingDim = 200\n",
    "embeddingMatrix = None\n",
    "\n",
    "if not os.path.isfile(f'glove.6B.{embeddingDim}d.txt'):\n",
    "  getGlove()\n",
    "\n",
    "if os.path.isfile(f'embeddingMatrix.{embeddingDim}d.pkl'):\n",
    "  with open(f'embeddingMatrix.{embeddingDim}d.pkl', 'rb') as f:\n",
    "    embeddingMatrix = pickle.load(f)\n",
    "\n",
    "  print('loaded matrix')\n",
    "else:\n",
    "  embeddingMatrix = generateMatrix(embeddingDim)\n",
    "\n",
    "  with open(f'embeddingMatrix.{embeddingDim}d.pkl', 'ab') as f:\n",
    "    pickle.dump(embeddingMatrix, f)\n",
    "\n",
    "  print('saved matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "90abb6e6-b570-4164-ad36-1d5377b99bdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxLen = 128\n",
    "\n",
    "X1array = []\n",
    "X2array = []\n",
    "YArray = []\n",
    "\n",
    "for key, hypoAndLabels in dataDict.items():\n",
    "\timg = f'./A2_Images/{key}.jpg'\n",
    "\n",
    "\tfor hypo, label in hypoAndLabels:\n",
    "\t\tlabel = tf.convert_to_tensor([labelTuple.index(label)])\n",
    "\t\tlabel.set_shape([1])\n",
    "\n",
    "\t\thypo = [vocabIndex[word] for word in hypo]\n",
    "\t\thypo = tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences([hypo], maxlen=maxLen)[0])\n",
    "\t\thypo.set_shape([maxLen])\n",
    "\n",
    "\t\tX1array.append(img)\n",
    "\t\tX2array.append(hypo)\n",
    "\t\tYArray.append(label)\n",
    "\n",
    "X1Numpy = np.array(X1array)\n",
    "X2Numpy = np.array(X2array, dtype='uint16')\n",
    "YNumpy = np.array(YArray, dtype='uint8')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({'image': X1Numpy, 'text': X2Numpy}, YNumpy))\n",
    "\n",
    "imageChannels = 3\n",
    "imageRes = 256\n",
    "patchesPerImage = 8\n",
    "\n",
    "patchRes = int(imageRes / patchesPerImage) \n",
    "patchNum = patchesPerImage ** 2 * imageChannels \n",
    "\n",
    "def getImage(path):\n",
    "\timg = tf.io.read_file(path)\n",
    "\timg = tf.io.decode_image(img, channels=imageChannels, dtype=tf.float32)\n",
    "\timg = tf.image.resize(img, (imageRes, imageRes))\n",
    "\timg = tf.reshape(img, (1, imageRes, imageRes, imageChannels))\n",
    "\timg = tf.image.extract_patches(\n",
    "\t\timages=img,\n",
    "\t\tsizes=[1, patchesPerImage, patchesPerImage, 1],\n",
    "\t\tstrides=[1, patchesPerImage, patchesPerImage, 1],\n",
    "\t\trates=[1, 1, 1, 1],\n",
    "\t\tpadding='VALID'\n",
    "\t)\n",
    "\timg = tf.reshape(img, (patchRes ** 2, patchNum))\n",
    "\n",
    "\treturn img\n",
    "\n",
    "def getImageWrapper(x, y):\n",
    "\timg = tf.py_function(func=getImage, inp=[x['image']], Tout=tf.float32)\n",
    "\timg.set_shape([patchRes ** 2, patchNum,])\n",
    "\n",
    "\tx['image'] = img \n",
    "\treturn x, y\n",
    "\n",
    "dataset = dataset.map(getImageWrapper, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fc482eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "192\n",
      "({'image': <tf.Tensor: shape=(1024, 192), dtype=float32, numpy=\n",
      "array([[0.21750155, 0.45987827, 0.6347683 , ..., 0.19901885, 0.45414293,\n",
      "        0.6514545 ],\n",
      "       [0.16561824, 0.43893322, 0.6393315 , ..., 0.8262642 , 0.7975571 ,\n",
      "        0.72094053],\n",
      "       [0.829873  , 0.82487917, 0.748381  , ..., 0.81371564, 0.78626466,\n",
      "        0.7117548 ],\n",
      "       ...,\n",
      "       [0.12718633, 0.13094592, 0.13310924, ..., 0.5203906 , 0.17092675,\n",
      "        0.26866114],\n",
      "       [0.11916474, 0.11280201, 0.14267261, ..., 0.70257074, 0.22066449,\n",
      "        0.27169263],\n",
      "       [0.64206034, 0.50968987, 0.4923476 , ..., 0.5946818 , 0.5394631 ,\n",
      "        0.50480247]], dtype=float32)>, 'text': <tf.Tensor: shape=(128,), dtype=uint16, numpy=\n",
      "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 5401,\n",
      "        193, 1840, 3350, 3606, 2326, 4552, 7595], dtype=uint16)>}, <tf.Tensor: shape=(1,), dtype=uint8, numpy=array([0], dtype=uint8)>)\n"
     ]
    }
   ],
   "source": [
    "print(patchRes)\n",
    "print(patchNum)\n",
    "\n",
    "for i in dataset.take(1):\n",
    "\tprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "692ba090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data batches 20\n",
      "val data batches 20\n",
      "train data batches 353\n",
      "ratios test:1956 val:1956 train:35217\n"
     ]
    }
   ],
   "source": [
    "testSize = int(datasetLen * 0.05)\n",
    "valSize = int(datasetLen * 0.05)\n",
    "trainSize = int(datasetLen - testSize - valSize)\n",
    "batchSize = 100\n",
    "\n",
    "def optimize(ds):\n",
    "\tds = ds.batch(batchSize) \n",
    "\tds = ds.cache() \n",
    "\tds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\t\n",
    "\treturn ds\n",
    "\n",
    "def getTest(ds):\n",
    "\tds = ds.take(testSize) \n",
    "\tds = optimize(ds)\n",
    "\t\n",
    "\treturn ds\n",
    "\n",
    "def getVal(ds):\n",
    "\tds = ds.skip(testSize)\n",
    "\tds = ds.take(valSize) \n",
    "\tds = optimize(ds)\n",
    "\n",
    "\treturn ds\n",
    "\n",
    "def getTrain(ds):\n",
    "\tds = ds.skip(valSize + testSize)\n",
    "\tds = ds.take(trainSize)\n",
    "\tds = optimize(ds)\n",
    "\n",
    "\treturn ds\n",
    "\n",
    "testDS = getTest(dataset)\n",
    "valDS = getVal(dataset)\n",
    "trainDS = getTrain(dataset)\n",
    "\n",
    "\n",
    "print(f\"test data batches {tf.data.experimental.cardinality(testDS).numpy()}\")\n",
    "print(f\"val data batches {tf.data.experimental.cardinality(valDS).numpy()}\")\n",
    "print(f\"train data batches {tf.data.experimental.cardinality(trainDS).numpy()}\")\n",
    "print(f'ratios test:{testSize} val:{valSize} train:{trainSize}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a6a8b",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "ToDo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "37e03aab-fb34-41c0-8006-0ee2da2d3297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_21\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_21\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,854,800</span> │ text[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_encoder_54 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1025</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">233,856</span> │ image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionEncoder</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_encoder_19 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">201</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span> │ reshape_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionEncoder</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_26      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1025</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,234,688</span> │ position_encoder… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)       │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">201</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">527,488</span> │ position_encoder… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ position_encoder… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_194 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1025</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │ transformer_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_198 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">201</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoderInput        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1226</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_194[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_198[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_28      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1226</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">149,504</span> │ decoderInput[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)       │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_201 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ dense_201[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m200\u001b[0m)  │  \u001b[38;5;34m1,854,800\u001b[0m │ text[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ image (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m192\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_encoder_54 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1025\u001b[0m, \u001b[38;5;34m192\u001b[0m) │    \u001b[38;5;34m233,856\u001b[0m │ image[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mPositionEncoder\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_encoder_19 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m201\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m42,240\u001b[0m │ reshape_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mPositionEncoder\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_26      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1025\u001b[0m, \u001b[38;5;34m192\u001b[0m) │  \u001b[38;5;34m1,234,688\u001b[0m │ position_encoder… │\n",
       "│ (\u001b[38;5;33mTransformer\u001b[0m)       │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m201\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m527,488\u001b[0m │ position_encoder… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ position_encoder… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_194 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1025\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m12,352\u001b[0m │ transformer_26[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_198 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m201\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │      \u001b[38;5;34m8,256\u001b[0m │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoderInput        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1226\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_194[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_198[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_28      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1226\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │    \u001b[38;5;34m149,504\u001b[0m │ decoderInput[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformer\u001b[0m)       │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ transformer_28[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_201 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m16,640\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ dense_201[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,080,081</span> (15.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,080,081\u001b[0m (15.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,225,281</span> (8.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,225,281\u001b[0m (8.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,854,800</span> (7.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,854,800\u001b[0m (7.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "denseStrides = 64\n",
    "\n",
    "class PositionEncoder(tf.keras.Layer):\n",
    "\tdef __init__(self, patchRes, patchNum):\n",
    "\t\tsuper(PositionEncoder, self).__init__()\n",
    "\t\tself.patchRes = patchRes\n",
    "\t\tself.patchNum = patchNum\n",
    "\t\tw_init = tf.random_normal_initializer()\n",
    "\t\tclassToken = w_init(shape=(1, patchNum), dtype=tf.float32)\n",
    "\t\tself.classToken = tf.Variable(initial_value=classToken, trainable=True)\n",
    "\t\tself.projection =tf.keras.layers.Dense(units=patchNum)\n",
    "\t\tself.positionEmbedding = tf.keras.layers.Embedding(input_dim=patchRes+1, output_dim=patchNum, mask_zero=True)\n",
    "\n",
    "\tdef call(self, patch):\n",
    "\t\tbatch = tf.shape(patch)[0]\n",
    "\t\tclassToken = tf.tile(self.classToken, multiples = [batch, 1])\n",
    "\t\tclassToken = tf.reshape(classToken, (batch, 1, self.patchNum))\n",
    "\t\tpatchesEmbed = self.projection(patch)\n",
    "\t\tpatchesEmbed = tf.concat([patchesEmbed, classToken], 1)\n",
    "\t\tpositions = tf.range(start=0, limit=self.patchRes+1, delta=1)\n",
    "\t\tpositionsEmbed = self.positionEmbedding(positions)\n",
    "\t\tencoded = patchesEmbed + positionsEmbed\n",
    "\t\treturn encoded\n",
    "\n",
    "class Transformer(tf.keras.Layer):\n",
    "\tdef __init__(self, dim, denseSize, heads=8, dropout=0.1):\n",
    "\t\tsuper(Transformer, self).__init__()\n",
    "\t\tself.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\t\tself.attn = tf.keras.layers.MultiHeadAttention(num_heads=heads, key_dim=dim, dropout=dropout)\n",
    "\t\tself.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\t\tself.dense1 = tf.keras.layers.Dense(denseSize, activation='relu')\n",
    "\t\tself.dense2 = tf.keras.layers.Dense(dim, activation='relu')\n",
    "\t\tself.drop = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "\tdef call(self, inputLayers):\n",
    "\t\tattention = self.attn(inputLayers, inputLayers)\n",
    "\t\tattention = tf.keras.layers.Add()([attention, inputLayers])\n",
    "\t\tattention = self.norm1(attention)\n",
    "\n",
    "\t\tmlp = self.drop(attention)\n",
    "\t\tmlp = self.dense1(mlp)\n",
    "\t\tmlp = self.drop(mlp)\n",
    "\t\tmlp = self.dense2(mlp)\n",
    "\t\tmlp = tf.keras.layers.Add()([mlp, attention])\n",
    "\t\tmlp = self.norm2(mlp)\n",
    "\n",
    "\t\treturn mlp\n",
    "\n",
    "# Image Encoder \n",
    "\n",
    "inputImageLayer = tf.keras.layers.Input(shape=(patchRes**2, patchNum), name='image')\n",
    "embeddingImage = PositionEncoder(patchRes**2, patchNum)(inputImageLayer)\n",
    "\n",
    "transformerImage = Transformer(patchNum, 128)(embeddingImage)\n",
    "\n",
    "imageOut = tf.keras.layers.Dense(denseStrides, activation='relu')(transformerImage)\n",
    "\n",
    "\n",
    "# Text Encoder \n",
    "inputTextLayer = tf.keras.layers.Input(shape=(maxLen,), name='text')\n",
    "\n",
    "embeddingText = tf.keras.layers.Embedding(len(vocab), embeddingDim, weights=[embeddingMatrix], trainable=False, name='embedding')(inputTextLayer)\n",
    "reshapeText = tf.keras.layers.Reshape((embeddingDim, maxLen))(embeddingText)\n",
    "embeddingPositionText = PositionEncoder(embeddingDim, maxLen)(reshapeText)\n",
    "\n",
    "transformerText = Transformer(maxLen, 128)(embeddingPositionText)\n",
    "\n",
    "textOut = tf.keras.layers.Dense(denseStrides, activation='relu')(attentionText)\n",
    "\n",
    "# Entanglement decoder \n",
    "decoderInput = tf.keras.layers.Concatenate(-2, name='decoderInput')([imageOut, textOut])\n",
    "\n",
    "transformerDecode = Transformer(denseStrides, 128)(decoderInput)\n",
    "\n",
    "flat = tf.keras.layers.GlobalAveragePooling1D()(transformerDecode)\n",
    "dense = tf.keras.layers.Dense(256, activation='relu')(flat)\n",
    "decoderOutput = tf.keras.layers.Dense(1, name='output', activation='sigmoid')(dense)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputImageLayer, outputs=decoderOutput)\n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "739d7062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m136784566597280\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs={'image': 'tf.Tensor(shape=(None, 1024, 192), dtype=float32)', 'text': 'tf.Tensor(shape=(None, 128), dtype=uint16)'}\\n  • training=True\\n  • mask={'image': 'None', 'text': 'None'}\\n  • kwargs=<class 'inspect._empty'>\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtrainDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatchSize\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deep/deep-learning/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/deep/deep-learning/.venv/lib/python3.10/site-packages/keras/src/ops/function.py:214\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[0;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[1;32m    212\u001b[0m output_tensors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs:\n\u001b[0;32m--> 214\u001b[0m     output_tensors\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtensor_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mpack_sequence_as(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_struct, output_tensors)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Exception encountered when calling Functional.call().\\n\\n\\x1b[1m136784566597280\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs={'image': 'tf.Tensor(shape=(None, 1024, 192), dtype=float32)', 'text': 'tf.Tensor(shape=(None, 128), dtype=uint16)'}\\n  • training=True\\n  • mask={'image': 'None', 'text': 'None'}\\n  • kwargs=<class 'inspect._empty'>\""
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "history = model.fit(\n",
    "  trainDS,\n",
    "\tvalidation_data=valDS,\n",
    "  epochs=8,\n",
    "  batch_size=batchSize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c7e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./final.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
